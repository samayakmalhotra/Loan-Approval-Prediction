# -*- coding: utf-8 -*-
"""Loan_approval_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16UhU2kot7NQgzcPOy_MXTLv-FH4b8kp3

# **LOAN PREDICTION**
"""

#Importing Libraries and packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/credit_data.csv")

#Show all properties on display and set style
pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')
warnings.simplefilter("ignore")

df.head(5)

"""# **Data Cleaning and Dealing with NULL values**"""

#Dropping first column as its not adding any value to our data
df.drop(columns=['Unnamed: 0'], inplace=True)

#Checking for null values
null_values = df.isnull().sum()
print(null_values)

"""Dealing with missing values"""

df['credit_amount'].mode()

df['credit_amount'].fillna(df['credit_amount'].mode()[0], inplace=True)

df.drop(columns=['other_parties'], inplace=True)

#Dropping ASMN column as it doesnt add any value to dataset
df.drop(columns=["asnm"], inplace=True)

#Category mapping for Checking status and NULL values to -1
category_mapping = {'<0DM': 0, '0_to_200DM': 1, '>200DM': 2, np.nan: -1}

# Create a new column with the encoded values
df['checking_status'] = df['checking_status'].map(category_mapping)

# Display the result
print(df[['checking_status']])

null_values = df.isnull().sum()
print(null_values)

#Checking data types of all columns
df.info()

print('Unique Values of Employment are', df['employment'].unique())

print('\nUnique Values of Credit History are', df['credit_history'].unique())

print('\nUnique Values of Purpose are', df['purpose'].unique())

print('\nUnique Values of Personal Status are', df['personal_status'].unique())

print('\nUnique Values of saving status are', df['savings_status'].unique()) #Fix Unknown_or_No savings text.

print('\nUnique Values of Job are', df['job'].unique())

print('\nUnique Values of Housing are', df['housing'].unique())

print('\nUnique Values of Other Payment plans are', df['other_payment_plans'].unique())

print('\nUnique Values of Housing are', df['property_magnitude'].unique())

print('Unique Values of Checking status are', df['checking_status'].unique())

#Replacing all the unknown values with -1
df['employment'].replace('unemployed', -1, inplace=True)
df['savings_status'].replace('Unknown_or_no_savings_acct', -1, inplace=True)
df['property_magnitude'].replace('unknown/no_property', -1, inplace=True)

# Verifying unique values in the "Employement","Savings" and "Property Magnitude" column
print("Unique Values of Employment after replacement:", df['employment'].unique())
print("Unique Values of Savings status after replacement:", df['savings_status'].unique())
print("Unique Values of Property magnitude after replacement:", df['property_magnitude'].unique())

#Creating an Age groups for the Age Column
conditions = [
    df.age < 25,
    (df.age >= 25) & (df.age <= 34),
    (df.age >= 35) & (df.age <= 49),
    df.age >= 50
]
tags = ["under 25", "25-34", "35-49", "50+"]
df["age_group"] = np.select(conditions, tags)
df.age_group.value_counts()

# Replace 'None' with 'No' in the 'own_telephone' column
df['own_telephone'].replace('none', 'no', inplace=True)

df.head()

"""# **EXPLORATORY DATA ANALYSIS**"""

Categorical_Col = []
Numerical_Col = []
for col in df.columns:
    if df[col].dtype == 'object':
        Categorical_Col.append(col)

    else:
        Numerical_Col.append(col)

Numerical_Col

Categorical_Col

import matplotlib.pyplot as plt
value_counts = df['accepted'].value_counts() #1- Accepted. 0- rejected

#Creating a pie chart
fig, ax = plt.subplots()
ax.pie(value_counts, labels=value_counts.index, autopct="%1.1f%%", startangle=90)

#Adding title and show the plot
ax.set_title("Distribution of Application Approval")
plt.show()

#Gender vs Loan Approval
df["gender"] = df.personal_status.apply (lambda x: x.split("_")[0])

approval_rate = df.pivot_table(values="accepted", index="gender",aggfunc=lambda x: 100*x.mean()) #this code first extracts gender information from the "personal_status" column and stores it in a new column called "gender". Then, it calculates the approval rate based on gender, aggregating the "accepted" column values.

plt.figure(figsize=(7,3))
sns.barplot(data=approval_rate,x="accepted",y=approval_rate.index,
            palette=["pink", "lightblue"])
for i, rate in enumerate (approval_rate.accepted):
    plt.text(rate-12,i,f"{round (rate, 1)}%", fontsize=30, color="white",va="center")

plt.title("Approval Rates by Gender")
plt.ylabel(None)
plt.xlabel("Approval Rate (%)")
plt.tight_layout ()
plt.show()

"""Loan Approval Status: About 1/3rd of applicants have been granted loan.
Sex: There are more Men than Women (approx. 2x) and more females are getting the loans approved
Martial Status: About 50% of the population in the dataset is Single men; Single men are more likely to be granted loans.

Employment: 5/6th of population is not self employed.
Property Area: More applicants from Semi-urban and also likely to be granted loans.
Applicant with credit history are far more likely to be accepted.
Loan Amount Term: Majority of the loans taken are for 360 Months (30 years).
"""

#Calculating the frequency of unique items
frequency = df['gender'].value_counts()

print("No of Males and Females:")
print(frequency)

#Calculating the frequency of unique items
frequency = df['personal_status'].value_counts()

print("Frequency of unique items:")
print(frequency)

"""
**Personal Status vs Loan Approval**

**Question: Do married men and females get loans approved more than single men?**"""

accepted_df = df[df['accepted'] == 1]
# Calculate the frequency of each personal status
status_frequency = accepted_df['personal_status'].value_counts()

# Display the frequency of each personal status
print(status_frequency)

# Calculate the frequency of each personal status
status_frequency = accepted_df['personal_status'].value_counts()

# Plot the pie chart
plt.figure(figsize=(8, 6))
plt.pie(status_frequency, labels=status_frequency.index, autopct='%1.1f%%', startangle=140)
plt.title('Frequency of Personal Status (Accepted Loans)')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.show()

"""Based on the Pie chart generated its clearly visible that our hypothesis was false. More single men get loans approved."""

# Filter the DataFrame to include only rows where 'personal_status' is 'male_single'
male_single_df = df[df['personal_status'] == 'male_single']

# Create a contingency table for 'checking_status' and 'accepted'
contingency_table = pd.crosstab(male_single_df['checking_status'], male_single_df['accepted'])

# Map legend labels
legend_labels = {0: 'Rejected', 1: 'Accepted'}

# Plot the stacked bar chart
contingency_table.plot(kind='bar', stacked=True, figsize=(10, 6))

plt.title('Relationship between "male_single" and "checking_status"')
plt.xlabel('Checking Status')
plt.ylabel('Count')
plt.legend(title='Loan Status', labels=[legend_labels[label] for label in contingency_table.columns])
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

columns_chart = ["duration","accepted", "age", "credit_amount"]

vars_for_chart = df.columns.difference(columns_chart)
fig, ax = plt.subplots(len(vars_for_chart), 1, figsize=(9.4, 6.5 * len(vars_for_chart)))
ax = ax.flatten()

for i, cols in enumerate(vars_for_chart):
    df[cols].value_counts().plot(kind="pie", autopct="%.1f%%", pctdistance=0.78, wedgeprops=dict(width=0.4), ax=ax[i])
    ax[i].set_title(cols.upper(), fontweight="bold")
    ax[i].set_ylabel(None)

plt.tight_layout()
plt.show()

"""**FEATURE ENGINEERING AND DATA IMPUTATIONS**"""

#Calculating the frequency of unique items in Employment
frequency = df['employment'].value_counts()

print("Frequency of unique items:")
print(frequency)

#Calculating the frequency of unique items in Age Group
frequency = df['age_group'].value_counts()

print("Frequency of unique items:")
print(frequency)

#Plotting Credit Amount in different charts
#Histogram chart

plt.figure(figsize= (6.5,4))
sns.distplot (df.credit_amount)
plt.xlabel("Credit Amount (DM)")
plt.tight_layout ()
plt.show()

"""Credit History vs Loan Accepted"""

pd.crosstab(df["credit_history"], df["accepted"], margins=True) #1 accepted, 0 rejected

"""*It can be seen that if the customer has paid all the credit till date then the chances for getting loan approved is higher*

Relationship between Age and Credit_Amount
"""

#Category mapping
credit_history_mapping = {
    'Critical_acct_other_credits_existing': 0,
    'Existing_credits_paid_till_now': 1,
    'Delay_in_past': 2,
    'No_credits_taken_or_all_paid': 3
}


df['credit_history'] = df['credit_history'].map(credit_history_mapping)

# Display the result
print(df[['credit_history']])

savings_status_map = {
    -1: 0,
    '<100DM': 1,
    '100_to_500DM': 2,
    '500_to_1000DM': 3,
    '>1000DM': 4
}

# Map the 'savings_status' column using the dictionary
df['savings_status'] = df['savings_status'].map(savings_status_map)

# The 'savings_status' column now contains numerical values
print(df['savings_status'].head())

# One-hot encode the 'purpose' column
encoded_df = pd.get_dummies(df['purpose'], prefix='purpose')

# Concatenate the encoded columns with the original DataFrame
df = pd.concat([df, encoded_df], axis=1)

# Drop the original 'purpose' column
df = df.drop('purpose', axis=1)

employment_map = {
    -1: -1,
    '<1yr': 0,
    '1_to_4yrs': 1,
    '4_to_7yrs': 2,
    '>7yrs': 3,

}

# Map the 'employment' column using the dictionary
df['employment'] = df['employment'].map(employment_map)

# One-hot encode the 'personal_status' column
encoded_data= pd.get_dummies(df['personal_status'])

# Concatenate the encoded columns with the original DataFrame
df = pd.concat([df, encoded_data], axis=1)

# Drop the original 'personal_status' column
df = df.drop('personal_status', axis=1)

property_magnitude_map = {
    'real_estate': 1,
    'building_society_savings_agreement/life_insurance': 2,
     -1: 0,
    'car_or_other_nonsavings': 3,

}

df['property_magnitude'] = df['property_magnitude'].map(property_magnitude_map)

other_payment_map = {
    'none': -1,
    'bank': 1,
    'stores': 2,

}

df['other_payment_plans'] = df['other_payment_plans'].map(other_payment_map)

housing_map = {
    'own': 1,
    'for_free': 2,
    'rent': 3,

}

df['housing'] = df['housing'].map(housing_map)

df.head()

# Calculate correlation between age and credit_amount
correlation = df['age'].corr(df['credit_amount'])

# Plot age against credit_amount
plt.scatter(df['age'], df['credit_amount'])

# Set title with correlation value
plt.title(f"Correlation: {round(correlation, 3)}")

# Set labels for axes
plt.xlabel("Applicant Age")
plt.ylabel("Credit Amount (DM)")

# Hide legend
plt.legend().remove()

# Show the plot
plt.show()

"""Correlation between these 2 variables is approx ~0.035 which is a positive value but still means very weak linear relationship between the two. Scatter plot shows no linear relationship between the 2

# **Hypothesis Testing**

Hypothesis: Applicants under the age of 25 request for less Loans
"""

#Plotting a Bar plot for "age_group" and "credit_amount"
plt.figure(figsize=(7,4))
sns.barplot(data=df, y="age_group", x="credit_amount", order=tags)
plt.xlabel("Credit Amount (DM)")
plt.ylabel("Age Group")
plt.tight_layout()
plt.show()
#thats almost true looking at the bar plot

from scipy import stats
#Significance level- 0.05
stats.ttest_ind(
    df.query("age_group == 'under 25'").credit_amount,
    df.query("age_group != 'under 25'").credit_amount
)

"""P-Value> 0.05. i.e There is no significant evidence stating that Age group under 25 and over 25 take almost the same loan amount.

Whatâ€™s the effect of owning a telephone on the likelihood of a credit application being accepted?
"""

#Chi-Square test will be used as we are comparing 2 categorical variables.
#Both owning a phone (yes & none) and credit approval (0 & 1) variables are categorical.
#Therefore our purpose will be making a hypothesis and test it with chi-square testing mechanism.

"""H0: Owning a phone has no effect on getting the credit approval
H1: Owning a phone affects getting the credit approval
"""

# Create a contingency table for 'own_telephone' and 'accepted'
data_crosstab = pd.crosstab(df["own_telephone"], df["accepted"], margins=True, margins_name="Total")

# Display the contingency table
print(data_crosstab)

"""Out of 404 participants owning a phone only 113 got the loan accepted(27%). and Out of 596 not owning a phone and 187 getting it accepted (31%). This shows that this data is probably from the 90's when people didnt own phones."""

###chi-square testing mechanism
alpha=0.05

chi_square=0
# Unique values in 'own_telephone' column
rows = df["own_telephone"].unique()

# Unique values in 'accepted' column
columns = df["accepted"].unique()

# Calculate degrees of freedom
dof = (len(rows) - 1) * (len(columns) - 1)

# Iterate through columns
for i in columns:
    # Iterate through rows
    for j in rows:
        observed = data_crosstab[i][j]
        expected = (data_crosstab[i]['Total'] * data_crosstab['Total'] [j]) / data_crosstab['Total']['Total']
        chi_square+= (observed-expected)**2/expected
chi_square

critical_value = stats.chi2.ppf (1-alpha, dof)
critical_value

"""# **DATA MODELLING**"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV
from sklearn.metrics import fbeta_score, make_scorer
import pandas as pd

df_copy = df.copy()
features = pd.get_dummies(df_copy)
target= df_copy.accepted
target = target.map({1:0,0:1})
features

# Perform one-hot encoding for JOB
df_copy= pd.get_dummies(df_copy, columns=['job'], prefix=['job'])

# Display the encoded DataFrame
print(df_copy.head())

import matplotlib.pyplot as plt
import seaborn as sns

abs_corr = abs(features.corr())

# Determine the top 20 highly correlated (negative or positive)
# features with the target variable
high_corr_features = abs_corr['accepted'].sort_values(ascending=False)
high_corr_features = high_corr_features.drop('accepted')
top_corr_features = high_corr_features.iloc[:18]

plt.figure(figsize=(10, 6))
sns.barplot(x=top_corr_features.values, y=top_corr_features.index, orient="horizontal")
plt.xlabel("Correlation Value", fontweight="bold")
plt.title("Top Correlated Features", fontweight="bold")
plt.tight_layout()
plt.show()

"""We will select the following columns: Duration, Credit_history, Checking status, Credit_amount, Housing, employment, Other_payment_plans, Age, age group, foriegn worker, gender and purpose"""

plt.figure(figsize=(6.15, 3))

sns.barplot(data=df, x="checking_status", y="accepted", order=['0', '1', '2', '-1'])

plt.ylabel("Approval Rate")
plt.xlabel("Checking Status")
plt.tight_layout()
plt.show()

"""This shows that customers with "<0DM" in their checking account get more loans approved."""

plt.figure(figsize=(6.15, 5))

sns.barplot(
    data=df_copy,
    x="checking_status",
    y="accepted",
    hue="savings_status",
    order=['0', '1', '2', '-1'], #<0DM, 100,200,-1
    hue_order=['1', '2', '3', '4', '0'] # hue_order=['<100DM', '100_to_500DM', '500_to_1000DM', '>1000DM', 'Unknown/no_savings_acct']
)

plt.ylim(0, 1.5)
plt.ylabel("Approval Rate")
plt.xlabel("Checking Status")
plt.tight_layout()
plt.show()

"""The checking status categories on the x-axis are:
0: indicates a checking account balance of less than 0 DM (Deutsche Mark).
1: indicates a checking account balance between 0 and 200 DM.
2: indicates a checking account balance greater than 200 DM.
-1: indicates no checking account or missing data.

The savings status categories in the legend are:
0: indicates a savings account balance of less than 100 DM.
1: indicates a savings account balance between 100 and 500 DM.
2: indicates a savings account balance between 500 and 1000 DM.
3: indicates a savings account balance greater than 1000 DM.
4: indicates no savings account or missing data.

Based on the chart, we can make the following inferences:

1. Having a higher savings account balance (over 1000DM, represented by savings_status = 3 in purple) generally leads to a higher approval rate across most checking status categories.

2. However, the approval rate for savings_status = 3 is not uniformly high across all checking status categories. It is highest for checking status 2 (balance > 200DM) and 1 (balance 0-200DM), but noticeably lower for checking status 0 (balance < 0DM) and -1 (no checking account/missing data).

3. This suggests that while having a substantial savings balance is advantageous, it does not guarantee approval if the checking account status is poor (negative balance or no account).

4. The combination of a high savings balance (>1000DM) and a reasonably positive checking account balance (>0DM) likely yields the highest approval rates based on this data.

So in summary, having over 1000DM in savings does increase the approval rate substantially, but the checking account status still plays an important role. A very high savings balance alone may not overcome the negative impact of a poor or non-existent checking account. Both savings and checking account balances seem to be considered for the approval decision based on the patterns in this chart.

Saving this data and running ML tasks on it.
"""

from google.colab import files

# Save the DataFrame to a CSV file
csv_filename = 'final dataset1.csv'
df_copy.to_csv(csv_filename, index=False)

# Download the CSV file to your local machine
files.download(csv_filename)